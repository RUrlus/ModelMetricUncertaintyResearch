{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we consider the estimation of the distribution of the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "from scipy.stats import levene\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mmur.simulators import LAlgorithm\n",
    "from mmur.generators import BlobGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumed is a Gaussian distribution for both classes. Dataset consists of two Gaussian blobs, with tunable covariance between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BlobGenerator(\n",
    "    train_size=1000,\n",
    "    test_size=200,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.8, 0.2],\n",
    "    flip_y=0.2,\n",
    "    class_d=2.0,\n",
    "    scale=1.0,\n",
    "    shuffle=False,\n",
    "    random_state=123,\n",
    "    var=1,\n",
    "    cov=0,\n",
    "    random_imbalance=False)\n",
    "X, y = generator.make_classification()\n",
    "\n",
    "generator.plot_blobs(X, y, scatter=True, contour=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we simulate the true confusion matrix distribution, over a randomly sampled train set (of fixed size) and test set of a fixed size.\n",
    "This is the same as the true distribution of the holdout estimator. From this we can also derive the true distributions of the performance metrics.\n",
    "\n",
    "Several components are added to resemble noise present in a practical setting. \n",
    "\n",
    "<b>random_imbalance</b>: given the class balance set by weights, the number observations with a certain label is sampled from a binomial distribution (multinomial for n_classes>2). Optional is to set a beta prior to this binomial distribution to increase the variance of the draws.\n",
    "\n",
    "<b>flip_y</b>: fraction of observations that is susceptible to being flipped. If an observation is susceptible it takes any of the labels with equal probability\n",
    "\n",
    "<b>fix_init</b>: fixes the seed for the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_CMs(train_size=5000, test_size=1000, weights=[0.8, 0.2], model_name='LR', class_d=2.0, random_imbalance=False, flip_y=0, imbalance_prior_a=None, imbalance_prior_b=None, tau=0.5, n_runs=5000, n_jobs=-2, random_state=123, fixed_data=False,data_seed=None):\n",
    "    \"\"\"Function to simulate multiple confusion matrices, either on independent train/test set samples, or using a fixed train/test sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_size : int, optional\n",
    "        Size of the train sample, by default 5000\n",
    "    test_size : int, optional\n",
    "        Size of the test sample, by default 1000\n",
    "    weights : list, optional\n",
    "        Mean class frequencies, by default [0.8, 0.2]\n",
    "    model_name : str, optional\n",
    "        Represents the name of the machine learning model, by default 'LR' for logistic regression\n",
    "    class_d : float, optional\n",
    "        Euclidean distance between the gaussian blob centers, by default 2.0\n",
    "    random_imbalance : bool, optional\n",
    "        Whether to have varying class frequencies per sample draw, by default False\n",
    "    flip_y : int, optional\n",
    "        Probabilility of a label being flipped, by default 0\n",
    "    imbalance_prior_a : _type_, optional\n",
    "        Prior parameter a for additional randomness in class frequencies, by default None\n",
    "    imbalance_prior_b : _type_, optional\n",
    "        Prior parameter b for additional randomness in class frequencies, by default None\n",
    "    tau : float, optional\n",
    "        Classification threshold, by default 0.5\n",
    "    n_runs : int, optional\n",
    "        Number of runs to perform, corresponds to number of output confusion matrices, by default 5000\n",
    "    n_jobs : int, optional\n",
    "        Number of threads for parallization, by default -2 (uses every CPU core except 1)\n",
    "    random_state : int, optional\n",
    "        random state to generate seeds for all the runs, by default 123\n",
    "    fixed_data : bool, optional\n",
    "        Whether to use a fixed dataset for each run, by default False\n",
    "    data_seed : _type_, optional\n",
    "        If `fixed_data`, seed for the data sample, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "\n",
    "    data_kwargs = {'train_size' : train_size, 'test_size' : test_size, 'weights':weights, 'class_d':class_d, 'random_imbalance': random_imbalance, 'flip_y' : flip_y, 'imbalance_prior_a': imbalance_prior_a, 'imbalance_prior_b' : imbalance_prior_b}\n",
    "\n",
    "    init_kwargs = {} # To be specified\n",
    "    fit_kwargs = {} # To be specified\n",
    "\n",
    "    kwargs = {'data_kwargs' : data_kwargs, 'init_kwargs' : init_kwargs, 'fit_kwargs' : fit_kwargs}\n",
    "    la = LAlgorithm(model_name,BlobGenerator,**kwargs)\n",
    "    cms = la.sim_true_cms(n_runs,tau,n_jobs,random_state,fixed_data=fixed_data,data_seed=data_seed)\n",
    "    return cms\n",
    "\n",
    "def create_and_plot(train_size, test_size, weights, flip_y, random_imbalance, imbalance_prior_a=None, imbalance_prior_b=None, contour=True, scatter=True):\n",
    "    generator = BlobGenerator(\n",
    "        train_size,\n",
    "        test_size=test_size,\n",
    "        weights=weights,\n",
    "        flip_y=flip_y,\n",
    "        random_imbalance=random_imbalance,\n",
    "        imbalance_prior_a=imbalance_prior_a,\n",
    "        imbalance_prior_b=imbalance_prior_b,\n",
    "        random_state=gen_seed,\n",
    "    )\n",
    "    X, y = generator.make_classification()\n",
    "    generator.plot_blobs(X, y, contour=contour, scatter=scatter)\n",
    "\n",
    "\n",
    "def cms_to_precision(cms):\n",
    "        \"\"\"\n",
    "        Computes the precisions based on a list of confusion matrices\n",
    "\n",
    "        Parameters\n",
    "        cms : list\n",
    "            List of (2 by 2) confusion matrices, where each confusion matrix is a list with two sublists\n",
    "            of length 2, containing the counts. [[true negatives,false positives],[false negatives,true positives]]\n",
    "\n",
    "        Returns\n",
    "        TP/(TP+FP) : ndarray\n",
    "            1D numpy array of precisions\n",
    "        \"\"\"\n",
    "\n",
    "        cms_array = np.array(cms)\n",
    "        TP = cms_array[:, 3]\n",
    "        FP = cms_array[:, 1]\n",
    "        return TP/(TP+FP)\n",
    "\n",
    "def cms_to_recall(cms):\n",
    "    \"\"\"\n",
    "    Computes the recalls based on a list of confusion matrices\n",
    "\n",
    "    Parameters\n",
    "    cms : list\n",
    "        List of(2 by 2) confusion matrices, where each confusion matrix is a list with two sublists\n",
    "        of length 2, containing the counts. [[true negatives, false positives], [false negatives, true positives]]\n",
    "\n",
    "    Returns\n",
    "    TP/(TP+FN) : ndarray\n",
    "        Vector of recalls\n",
    "    \"\"\"\n",
    "    cms_array = np.array(cms)\n",
    "    TP = cms_array[:, 3]\n",
    "    FN = cms_array[:, 2]\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def CM_dict_to_prec(CM_dict):\n",
    "    \"\"\"Function to convert dictionary of confusion matrices to dictionary of precisions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    CM_dict : dict containing np.ndarrays\n",
    "        each value is a np.ndarray of shape (n,4)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        identical keys, contain an np.ndarray for the same keys but now with precisions\n",
    "    \"\"\"\n",
    "    return {k: cms_to_precision(v) for k, v in CM_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_params(params, exp_nr, noise_name, comb_type='zip',save=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs experiments to evaluate the confusion matrix' distribution for multiple parameter settings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        Dictionary containing the names of the parameters with as dict values the parameter settings to evaluate.\n",
    "        Allows for both iterable and non-iterable dict values.\n",
    "    exp_nr : int or str\n",
    "        Experiment number, used for naming the file with results.\n",
    "    noise_name : str\n",
    "        Name of the type of noise that is investigated, used to name output file\n",
    "    comb_type : str, optional\n",
    "        The combination type of the parameters, either iterate over the zipped lists ('pairings'), or go through all combinations of the parameters, by default 'pairings'\n",
    "    save : bool, optional\n",
    "        Indicates whether to save the output into a pkl file, by default True\n",
    "    verbose : bool, optional\n",
    "        Whether to print the combination that is being computed at each iteration, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of results, of form '...'\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect the parameter that have constant values\n",
    "    constant_dict = {}\n",
    "    keys = list(params.keys())\n",
    "    values = list(params.values())\n",
    "    for k, v in zip(keys, values):\n",
    "        if not isinstance(v, list):\n",
    "            constant_dict[k] = params.pop(k)\n",
    "\n",
    "    if comb_type == 'zip':\n",
    "        combis = list(zip(*params.values()))\n",
    "    elif comb_type == 'product':\n",
    "        combis = list(it.product(*params.values()))\n",
    "    else:\n",
    "        raise ValueError('`comb_type` must be either `zip` or `product`')\n",
    "\n",
    "    print('Varied parameter values: ',params)\n",
    "    print('Constant parameter values: ',constant_dict)\n",
    "\n",
    "    results = {}\n",
    "    for i,values in enumerate(combis,1):\n",
    "        if verbose:\n",
    "            print(\n",
    "                f'Combination {i} out of {len(combis)} being tried: {list(zip(list(params), values))}')\n",
    "        result_key = ', '.join(['='.join([p, str(v)])\n",
    "                            for p, v in zip(list(params), values)])\n",
    "        results[result_key] = sim_CMs(**constant_dict,**dict(zip(params, values))) # input the constants and the varying parameter values using two dictionaries\n",
    "\n",
    "    if save:\n",
    "        with open(f'exp{exp_nr}_{noise_name}_noise_cm.pkl', 'wb') as f:\n",
    "            pkl.dump(results, f)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import levene\n",
    "\n",
    "def df_cols_levene(df):\n",
    "    pvalues = []\n",
    "    var1 = []\n",
    "    var2 = []\n",
    "    cols = []\n",
    "    for col1,col2 in it.combinations(df.columns,2):\n",
    "        var1.append(np.var(df[col1]))\n",
    "        var2.append(np.var(df[col2]))\n",
    "        pvalues.append(levene(df[col1], df[col2]).pvalue)\n",
    "        cols.append(col1 + ' vs ' +  col2)\n",
    "\n",
    "    data = np.round(np.array([pvalues,var1,var2]),6).T\n",
    "    return pd.DataFrame(data,index=cols,columns=[\"p-value levene's test\",'Variance 1', 'Variance 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Looking at the effect of sampling noise for the train and test set separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First consider only the effect of sampling noise for varying train en test sizes. Compute all combinations\n",
    "\n",
    "params = {\n",
    "    'train_size':[int(1e6),5000,500],\n",
    "    'test_size':[int(1e6),1000,100],\n",
    "    'n_runs' : 1000\n",
    "}\n",
    "\n",
    "results_smpl = run_params(params,1,'sampling',comb_type='product',save=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mean_std(values, decimals=3):\n",
    "    print(\"Mean precision of: \", np.round(np.mean(values), decimals=decimals))\n",
    "    print(\"Std of: \", np.round(np.std(values), decimals=decimals))\n",
    "\n",
    "print_mean_std(cms_to_precision(\n",
    "    results_smpl['train_size=1000000, test_size=1000000']))\n",
    "# We observe that the precision converges to 0.774 for large train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs_dict = {k: cms_to_precision(v) for k, v in results_smpl.items()}\n",
    "min_var = precs_dict.pop('train_size=1000000, test_size=1000000', None)\n",
    "sns.boxplot(data=pd.DataFrame(precs_dict), orient = 'h')\n",
    "plt.axvline(np.mean(min_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levene test for difference in variance\n",
    "df_cols_levene(pd.DataFrame(precs_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Simulate precision distribution with sampling noise and stochastic class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate the observed balances\n",
    "# The class balance is determined by the sample size, mean balance defined by weights,\n",
    "# and another weighing term \"imbalance_prior_frac\" to inflate/deflate variance.\n",
    "\n",
    "def sim_random_balance(generator, imbalance_prior_a, imbalance_prior_b, n_runs=100):\n",
    "    generator.imbalance_prior_a = imbalance_prior_a\n",
    "    generator.imbalance_prior_b = imbalance_prior_b\n",
    "    bal = []\n",
    "    for _ in range(n_runs):\n",
    "        _, y = generator.make_classification()\n",
    "        bal.append(np.mean(y))\n",
    "    return bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating the class balance distribution\n",
    "\n",
    "generator = BlobGenerator(\n",
    "    train_size=5000,\n",
    "    test_size=1000,\n",
    "    weights=[0.8,0.2],\n",
    "    random_imbalance=True,\n",
    "    )\n",
    "\n",
    "evidence = [None,100,1000]\n",
    "\n",
    "balance_distr = {}\n",
    "for e in evidence:\n",
    "    if e is None:\n",
    "        a, b = None, None\n",
    "    else:\n",
    "        a, b = int(e*0.8), int(e*0.2)\n",
    "    balance_distr[str(e)] = sim_random_balance(\n",
    "        generator, a, b, n_runs=5000)\n",
    "sns.displot(pd.DataFrame(balance_distr), kind='kde').set(title='Class balance distribution for different evidence parameters')\n",
    "plt.xlabel('Class balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the distribution of the precision for several degrees of uncertainty in the class balance, for two dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'random_imbalance': True,\n",
    "    'n_runs': 1000,\n",
    "    'imbalance_prior_a': [None, 800, 80]*2,\n",
    "    'imbalance_prior_b': [None, 200, 20]*2,\n",
    "    'train_size' : [5000]*3 + [500]*3,\n",
    "    'test_size' : [1000]*3 + [100]*3\n",
    "}\n",
    "results_rbal_size = run_params(params, 2, 'imbalance_datasize', comb_type='zip', save=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({k: cms_to_precision(v) for k, v in results_rbal_size.items()})\n",
    "sns.boxplot(data=df,orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols_levene(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the effect of random class balance if we increase the complexity of class separation by decreasing the distance between the blob centers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BlobGenerator(\n",
    "    train_size=5000,\n",
    "    test_size=1000,\n",
    "    weights=[0.8, 0.2],\n",
    "    class_d=1,\n",
    "    random_state=123\n",
    ")\n",
    "X, y = generator.make_classification()\n",
    "generator.plot_blobs(X, y, scatter=True, contour=False)\n",
    "\n",
    "generator = BlobGenerator(\n",
    "    train_size=5000,\n",
    "    test_size=1000,\n",
    "    weights=[0.8, 0.2],\n",
    "    class_d=1.5,\n",
    "    random_state=123\n",
    ")\n",
    "X, y = generator.make_classification()\n",
    "generator.plot_blobs(X, y, scatter=True, contour=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'random_imbalance': True,\n",
    "    'n_runs': 1000,\n",
    "    'imbalance_prior_a': [None, 800, 80]*2,\n",
    "    'imbalance_prior_b': [None, 200, 20]*2,\n",
    "    'class_d' : [1]*3 + [1.5]*3\n",
    "}\n",
    "results_rbal_classd = run_params(\n",
    "    params, 3, 'imbalance_datasize', comb_type='zip', save=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({k: cms_to_precision(v) for k, v in results_rbal_classd.items()})\n",
    "sns.boxplot(data=df,orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols_levene(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Introducing label noise to the data. What is the effect on the distribution of the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BlobGenerator(\n",
    "    train_size=5000,\n",
    "    test_size=1000,\n",
    "    weights=[0.8,0.2],\n",
    "    flip_y=0.1,\n",
    "    random_state=123,\n",
    "    )\n",
    "X, y = generator.make_classification()\n",
    "generator.plot_blobs(X, y, scatter=True, contour=False)\n",
    "\n",
    "generator = BlobGenerator(\n",
    "    train_size=5000,\n",
    "    test_size=1000,\n",
    "    weights=[0.8,0.2],\n",
    "    flip_y=0.3,\n",
    "    random_state=123)\n",
    "X, y = generator.make_classification()\n",
    "generator.plot_blobs(X, y, scatter=True, contour=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_runs' : 1000,\n",
    "    'flip_y' : [0,0.05,0.1,0.2]\n",
    "}\n",
    "\n",
    "results_yflip = run_params(params,4,'label','zip',save=True,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(CM_dict_to_prec(results_yflip))\n",
    "sns.boxplot(data=df,orient = 'h')\n",
    "sns.displot(df,kind='kde')\n",
    "\n",
    "df_cols_levene(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All noise components combined, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'flip_y':[0.1],\n",
    "    'random_imbalance':[True],\n",
    "    'n_runs' : 1000\n",
    "}\n",
    "\n",
    "results_combined = run_params(params,5,'combined','zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(data=cms_to_precision(results_smpl['train_size=5000, test_size=1000']))\n",
    "\n",
    "results_combined.update({\n",
    "    'train_size=5000, test_size=1000': results_smpl['train_size=5000, test_size=1000']})\n",
    "sns.boxplot(data=pd.DataFrame(CM_dict_to_prec(results_combined)),orient='h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols_levene(pd.DataFrame(CM_dict_to_prec(results_combined)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison for different types of noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Consider now two models with non-deterministic training: XGBoost and a neural net (MLP), what is the distribution of the precision on a single dataset due to non-deterministic training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_nd_train_prec(train_size, test_size, weights, model_name, n_runs, nd_train = True, flip_y=0, random_imbalance=False, gen_seed=123, tau=0.5, n_jobs=1):\n",
    "    generator = BlobGenerator(\n",
    "        train_size=train_size,\n",
    "        test_size=test_size,\n",
    "        weights=weights,\n",
    "        flip_y=flip_y,\n",
    "        random_imbalance=random_imbalance,\n",
    "        random_state=gen_seed,\n",
    "    )\n",
    "    LA = LAlgorithm(model_name, generator, nd_train=nd_train)\n",
    "    cms = LA.repeat_nd_train(n_runs, data_seed=gen_seed,\n",
    "                             tau=tau, n_jobs=n_jobs)\n",
    "    precs = LA._cms_to_precision(cms)\n",
    "    return precs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'fixed_data' : True,\n",
    "    'data_seed' : 1,\n",
    "    'n_runs' : 1000,\n",
    "    'model_name': ['LR', 'NN', 'XGB']\n",
    "}\n",
    "\n",
    "results_ndtrain = run_params(params,6,'nd_train','zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(CM_dict_to_prec(results_ndtrain))\n",
    "sns.displot(df,kde=True)\n",
    "plt.title('Precision distribution a fixed dataset')\n",
    "plt.xlabel('Precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df,orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Uncertainty for LR, XGB and NN for random train/test draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without additional noise components\n",
    "\n",
    "params = {\n",
    "    'random_state' : 123,\n",
    "    'n_runs' : 1000,\n",
    "    'model_name': ['LR', 'NN', 'XGB']\n",
    "}\n",
    "\n",
    "results_cleanmodels = run_params(params,6,'cleanmodels','zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(CM_dict_to_prec(results_cleanmodels))\n",
    "sns.displot(df,kde=True)\n",
    "\n",
    "df_cols_levene(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With additional noise components\n",
    "\n",
    "params = {\n",
    "    'random_imbalance' : True,\n",
    "    'flip_y' : 0.1,\n",
    "    'random_state' : 1,\n",
    "    'n_runs' : 1000,\n",
    "    'model_name': ['LR', 'NN', 'XGB']\n",
    "}\n",
    "\n",
    "results_noisymodels = run_params(params,6,'cleanmodels','zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(CM_dict_to_prec(results_noisymodels))\n",
    "sns.displot(df,kde=True)\n",
    "df_cols_levene(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Investigating the differences in variance for a NN depending on data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_var_prec_nd(train_size, test_size, weights, flip_y, random_imbalance, gen_seed, model_name, fix_init, n_runs, n_sets, tau=0.5, parallel=False):\n",
    "    generator = BlobGenerator(\n",
    "        train_size,\n",
    "        test_size=test_size,\n",
    "        weights=weights,\n",
    "        flip_y=flip_y,\n",
    "        random_imbalance=random_imbalance,\n",
    "        random_state=gen_seed,\n",
    "    )\n",
    "    LA = LAlgorithm(model_name, generator, fix_init=fix_init)\n",
    "    stds = LA.sim_prec_std(n_sets, n_runs, parallel=parallel)\n",
    "    return stds\n",
    "\n",
    "\n",
    "def sim_iv_prec_nd(train_size, test_size, weights, flip_y, random_imbalance, gen_seed, model_name, fix_init, n_runs, n_sets, qrange=0.95, tau=0.5, parallel=False):\n",
    "    generator = BlobGenerator(\n",
    "        train_size,\n",
    "        test_size=test_size,\n",
    "        weights=weights,\n",
    "        flip_y=flip_y,\n",
    "        random_imbalance=random_imbalance,\n",
    "        random_state=gen_seed,\n",
    "    )\n",
    "    LA = LAlgorithm(model_name, generator, fix_init=fix_init)\n",
    "    iv_lens = LA.sim_prec_iv(n_sets, n_runs, qrange, parallel, n_jobs=15)\n",
    "    return iv_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 5000\n",
    "test_size = 1000\n",
    "weights = [0.8, 0.2]\n",
    "n_runs = 100\n",
    "n_sets = 100\n",
    "flip_y = 0\n",
    "random_imbalance = False\n",
    "fix_init = False\n",
    "model_name = 'NN'\n",
    "gen_seed = 12\n",
    "\n",
    "iv_lens = sim_iv_prec_nd(\n",
    "    train_size,\n",
    "    test_size,\n",
    "    weights,\n",
    "    flip_y,\n",
    "    random_imbalance,\n",
    "    gen_seed,\n",
    "    model_name,\n",
    "    fix_init,\n",
    "    n_runs,\n",
    "    n_sets,\n",
    "    qrange=0.95,\n",
    "    tau=0.5,\n",
    "    parallel=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(iv_lens, kde=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.quantile(precs0,0.9)\n",
    "# sns.displot(precs1)\n",
    "# plt.vlines([np.quantile(precs1,0.025),np.quantile(precs1,0.975)],ymin=0,ymax=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% First we simulate the true CM distribution, over a randomly sampled train set (of fixed size) and test set of a fixed size\n",
    "# #This is the same as the true distribution of a holdout estimator\n",
    "# train_size = 1000\n",
    "# test_frac = 1/5\n",
    "# weights = [0.8,0.2]\n",
    "# n_sets = 100\n",
    "# random_imbalance = False\n",
    "# flip_y = 0\n",
    "\n",
    "# model_name = 'LR'\n",
    "# fix_init = True\n",
    "\n",
    "# repeats = 10\n",
    "\n",
    "# def simulate_cms(train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,fix_init,random_state,train_seed=0):\n",
    "#     data_dict = f.generate_blobs(train_size = train_size,test_size=int(test_frac*train_size),n_sets=n_sets,\n",
    "#                                 random_state = random_state,flip_y=flip_y,weights = weights,random_imbalance=random_imbalance)\n",
    "#     return f.holdout_CM(model_name,data_dict,train_seed,fix_init=fix_init)\n",
    "\n",
    "# cms = np.vstack(Parallel(n_jobs=-1,verbose = 3)(delayed(simulate_cms)(train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,\n",
    "#     fix_init,random_state=seed_i) for seed_i in range(repeats)))\n",
    "\n",
    "# # precisions\n",
    "# TP = cms[:,1,1]\n",
    "# FP = cms[:,0,1]\n",
    "# precs = TP/(TP+FP)\n",
    "\n",
    "# #recalls\n",
    "# FN = cms[:,1,0]\n",
    "# recs = TP/(TP+FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I experiment with different weight splits, to observe the effect on 3 models: Logistic regression, neural network and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% First we simulate the true CM distribution, over a randomly sampled train set (of fixed size) and test set of a fixed size\n",
    "# #This is the same as the true distribution of a holdout estimator\n",
    "\n",
    "# def flatten(t):\n",
    "#     return [item for sublist in t for item in sublist]\n",
    "\n",
    "# def simulate_distr(model_names, weight_splits,train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,fix_init,repeats):\n",
    "#     precs_list = []\n",
    "#     recs_list = []\n",
    "#     col_model = []\n",
    "#     col_balance = []\n",
    "\n",
    "#     for model_name in model_names:\n",
    "#         for w in weight_splits:\n",
    "#             weights = [1-w,w]\n",
    "#             cms = np.vstack(Parallel(n_jobs=-1,verbose = 1)(delayed(simulate_cms)(train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,\n",
    "#                 fix_init,random_state=seed_i) for seed_i in range(repeats)))\n",
    "\n",
    "#             # precisions\n",
    "#             TP = cms[:,1,1]\n",
    "#             FP = cms[:,0,1]\n",
    "#             precs = TP/(TP+FP)\n",
    "\n",
    "#             #recalls\n",
    "#             FN = cms[:,1,0]\n",
    "#             recs = TP/(TP+FN)\n",
    "#             precs_list.append(precs.tolist())\n",
    "#             recs_list.append(recs.tolist())\n",
    "#             col_model.append(np.repeat(model_name,n_sets*repeats).tolist())\n",
    "#             col_balance.append(np.repeat(w,n_sets*repeats).tolist())\n",
    "#     results = {'precision': flatten(precs_list), 'recall':flatten(recs_list),'model':flatten(col_model),'balance':flatten(col_balance)}\n",
    "#     return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['LR','NN','GB']\n",
    "# # weight_splits = [0.1,0.3,0.5]\n",
    "# train_sizes = [100,200,300]\n",
    "\n",
    "# train_size = 1000\n",
    "# test_frac = 1/5\n",
    "# n_sets = 10\n",
    "# random_imbalance = True\n",
    "# # flip_y = 0\n",
    "# fix_init = True\n",
    "# repeats = 1000\n",
    "\n",
    "# for flip_y in [0.05,0.1,0.2]:\n",
    "\n",
    "#     filename = ' '.join(['/ri',str(random_imbalance),'flip_y',str(flip_y),'fix_init',str(fix_init)])\n",
    "#     plot_dir = 'c:/Users/coenv/Documents/internship/ING/Thesis/code/ModelMetricUncertaintyResearch/results'\n",
    "\n",
    "#     results = simulate_distr(model_names, weight_splits,train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,fix_init,repeats)\n",
    "\n",
    "#     pd.DataFrame(results).to_csv(plot_dir + filename +'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First consider only the noise introduced due to random class balance. First we fix the (mean) balance at 0.3, and set the train size at 1000. Consider a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 500\n",
    "# test_frac = 1/5\n",
    "# weights = [0.7,0.3]\n",
    "# n_sets = 1\n",
    "# flip_y = 0\n",
    "\n",
    "# model_name = 'LR'\n",
    "# fix_init = True\n",
    "\n",
    "# repeats = 1000\n",
    "\n",
    "# random_imbalance = False\n",
    "# cms0 = np.vstack(Parallel(n_jobs=-1,verbose = 3)(delayed(simulate_cms)(train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,\n",
    "#     fix_init,random_state=seed_i) for seed_i in range(repeats)))\n",
    "\n",
    "# random_imbalance = True\n",
    "# cms0 = np.vstack(Parallel(n_jobs=-1,verbose = 3)(delayed(simulate_cms)(train_size,test_frac,weights,random_imbalance,n_sets,flip_y,model_name,\n",
    "#     fix_init,random_state=seed_i) for seed_i in range(repeats)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dir = 'c:/Users/coenv/Documents/internship/ING/Thesis/code/ModelMetricUncertaintyResearch/results'\n",
    "\n",
    "# train_size = 1000\n",
    "# test_frac = 1/5\n",
    "# n_sets = 10\n",
    "# random_imbalance = False\n",
    "# flip_y = 0\n",
    "# fix_init = True\n",
    "# repeats = 1000\n",
    "# filename = ' '.join(['/ri',str(random_imbalance),'flip_y',str(flip_y),'fix_init',str(fix_init)])\n",
    "# df_0 = pd.read_csv(plot_dir + filename +'.csv')\n",
    "\n",
    "# train_size = 1000\n",
    "# test_frac = 1/5\n",
    "# n_sets = 10\n",
    "# random_imbalance = True #random imbalance included\n",
    "# flip_y = 0\n",
    "# fix_init = True\n",
    "# repeats = 1000\n",
    "# filename = ' '.join(['/ri',str(random_imbalance),'flip_y',str(flip_y),'fix_init',str(fix_init)])\n",
    "# df_1 = pd.read_csv(plot_dir + filename +'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_0 = pd.DataFrame(results)\n",
    "# # sns.displot(df_0,x = 'precision',hue = 'model',col = 'balance',kde=True)\n",
    "# # df_0.to_csv('results0.csv')\n",
    "# # df\n",
    "# def make_plot(df,model,balance,metric='precision'):\n",
    "#     df_slice = df[(df['balance']==balance)& (df['model']==model)]\n",
    "#     sns.displot(data=df_slice,x=metric)\n",
    "#     plt.title(' '.join([model + ',','balance=' + str(balance)]))\n",
    "# # sns.displot(data = df_0[(df_0['balance']==0.3)&(df_0['model']=='LR')],x = 'precision')\n",
    "# # plt.title(' '.join(['LR,','balance=' + str(0.3)]))\n",
    "\n",
    "# # make_plot(df_0,'LR',0.1,metric='recall')\n",
    "# make_plot(df_0,'GB',0.3,metric='precision')\n",
    "# make_plot(df_1,'GB',0.3,metric='precision')\n",
    "# # make_plot(df_0,'LR',0.5,metric='recall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(np.array(precs_list).T)\n",
    "# np.array(precs_list).flatten().shape\n",
    "# col_model = np.repeat(model_names,repeats*n_sets*len(weight_splits)).shape\n",
    "# col_balance =\n",
    "# # cms.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.cm as cm\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# H, xedges, yedges= np.histogram2d(precs,recs)\n",
    "\n",
    "\n",
    "# xpos, ypos = np.meshgrid(xedges[:-1], yedges[:-1], indexing=\"ij\")\n",
    "# xpos = xpos.ravel()\n",
    "# ypos = ypos.ravel()\n",
    "# zpos = 0\n",
    "\n",
    "# # Construct arrays with the dimensions for the 16 bars.\n",
    "# dx = dy = 0.5 * np.ones_like(zpos)\n",
    "# dz = H.ravel()\n",
    "\n",
    "# cmap = cm.get_cmap('jet') # Get desired colormap - you can change this!\n",
    "# max_height = np.max(dz)   # get range of colorbars so we can normalize\n",
    "# min_height = np.min(dz)\n",
    "# # scale each z to [0,1], and get their rgb values\n",
    "# rgba = [cmap((k-min_height)/max_height) for k in dz]\n",
    "\n",
    "# ax.bar3d(xpos, ypos, zpos, dx, dy, dz, zsort='average',color = rgba)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the true precisions and recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # precs\n",
    "# TP = cms[:,1,1]\n",
    "# FP = cms[:,0,1]\n",
    "# precs = TP/(TP+FP)\n",
    "\n",
    "# #recs\n",
    "# FN = cms[:,1,0]\n",
    "# recs = TP/(TP+FN)\n",
    "\n",
    "# sns.displot(precs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(precs,recs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method by Caelen (2017), multinomial likelihood with Dirichlet priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Given an observed CM, compute the distribution of a performance metric by MC-sampling\n",
    "# from numpy.random import default_rng\n",
    "\n",
    "# theta_prior = np.ones(4)\n",
    "# rng = default_rng(0)\n",
    "# cm = cms[0]\n",
    "\n",
    "# def sample_CM(v,theta_prior):\n",
    "#     theta_post = rng.dirichlet(alpha = v + theta_prior)\n",
    "#     cm = rng.multinomial(n = v.sum(), pvals = theta_post) # pakketten die dit sneller doen\n",
    "#     return cm.tolist()\n",
    "\n",
    "# def sample_CM_N(v,theta_prior,MC_N=1000):\n",
    "#     CL_cms = np.zeros((MC_N,4),dtype = int)\n",
    "#     for i in range(MC_N):\n",
    "#         CL_cms[i] = sample_CM(v,theta_prior)\n",
    "#     return CL_cms\n",
    "\n",
    "# def prec_from_CMs(cms):\n",
    "#     TP = cms[:,-1]\n",
    "#     FP = cms[:,1]\n",
    "#     return TP/(TP+FP)\n",
    "\n",
    "# def rec_from_CMs(cms):\n",
    "#     TP = cms[:,-1]\n",
    "#     FN = cms[:,-2]\n",
    "#     return TP/(TP+FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cms_CA = sample_CM_N(cm.flatten(),theta_prior,MC_N=1000)\n",
    "# Cae_precs = []\n",
    "\n",
    "# for cm in cms[:3]:\n",
    "#     cms_CA = sample_CM_N(cm.flatten(),theta_prior,MC_N=1000)\n",
    "#     Cae_precs.append(prec_from_CMs(cms_CA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(Cae_precs[2], label = 'Estimated distribution')\n",
    "# sns.distplot(precs, label = 'Observed/real distribution')\n",
    "# plt.legend()\n",
    "# plt.title('Distribution of precision')\n",
    "\n",
    "# #TODO: Calculate the coverage and size of confidence intervals created by Caelen's method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def CM_cv(X,y,test_frac,model_name,seed=0,fix_train = False):\n",
    "#     kf = KFold(n_splits = int(1/test_frac)+1)\n",
    "#     LA = LAlgorithm(model_name)\n",
    "#     cms = []\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         if not fix_train:\n",
    "#             seed+=1\n",
    "#         model = LA.init_model(random_state = seed) #initialize model\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#         model.fit(X_train,y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         cms.append(confusion_matrix(y_test,y_pred).tolist())\n",
    "#     return cms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = f.generate_blobs(train_size = 1000,test_size=200,n_sets=n_sets,random_state = 1,flip_y=0.1)\n",
    "# X,y = f.unpack_data_dict(data_dict,stack = True)\n",
    "# cms_cv = CM_cv(X[0],y[0],test_frac,model_name)\n",
    "# np.array(cms_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CM_cv(X_train[0],y,test_frac,seed,model_name,fix_train = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, consider the case where we have only a single train-test run. This is equivalent to a holdout estimator. \n",
    "To obtain a confidence interval we need to evaluate the variance of this estimator. \n",
    "\n",
    "This can be done by:\n",
    "1.  assuming a binomial distribution on the precision/recall and using a normal approximation for the confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% To obtain a confidence interval of the expected precision we need to evaluate the variance of the holdout estimator. This can be done by assuming\n",
    "# #a binomial distribution and using a normal approximation for the confidence interval\n",
    "# n_pos = out[:,2,:].flatten()\n",
    "\n",
    "# l = true_precs - 1.96*np.sqrt((true_precs * (1-true_precs))/n_pos)\n",
    "# u = true_precs + 1.96*np.sqrt((true_precs * (1-true_precs))/n_pos)\n",
    "\n",
    "# #fraction of confidence intervals that cover the true population parameter value\n",
    "# print(np.mean((l<EP_prec) & (EP_prec<u)))\n",
    "\n",
    "# #We observe that the confidence interval is approximately correct, it covers the true mean in about 94 percent of the cases.\n",
    "\n",
    "# #Save the widths to compare with the other methods\n",
    "# holdout_CI_width = u-l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. bootstrapping both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy.random import default_rng\n",
    "\n",
    "# def bootstrap_sample(learn_algo,data_dict,seed=0,fix_init=False):\n",
    "#     rng = default_rng(seed)\n",
    "\n",
    "#     X_train = data_dict['train']['X']\n",
    "#     y_train = data_dict['train']['y']\n",
    "#     X_test = data_dict['test']['X']\n",
    "#     y_test = data_dict['test']['y']\n",
    "\n",
    "#     X = np.hstack([X_train, X_test])\n",
    "#     y = np.hstack([y_train, y_test])\n",
    "\n",
    "#     n_obs = y.shape[1]\n",
    "#     bs_indices = rng.integers(0,high = n_obs,size = y.shape)\n",
    "\n",
    "#     X_bs = np.zeros(X.shape)\n",
    "#     y_bs = np.zeros(y.shape)\n",
    "\n",
    "#     for i in range(len(y_bs)):\n",
    "#         X_bs[i] = X[i][bs_indices[i]]\n",
    "#         y_bs[i] = y[i][bs_indices[i]]\n",
    "\n",
    "#     train_size = y_train.shape[1]\n",
    "\n",
    "#     X_train = X_bs[:,:train_size,:]\n",
    "#     X_test = X_bs[:,train_size:,:]\n",
    "#     y_train = y_bs[:,:train_size,:]\n",
    "#     y_test = y_bs[:,train_size:,:]\n",
    "\n",
    "#     return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "# def bootstrap_prec_rec(learn_algo,data_dict,seed=0,fix_init=False):\n",
    "#     X_train,X_test,y_train,y_test = bootstrap_sample(learn_algo,data_dict,seed=0,fix_init=False)\n",
    "\n",
    "#     if fix_init:\n",
    "#         train_seed = 0\n",
    "#     else:\n",
    "#         train_seed = seed\n",
    "#     LA = LAlgorithm(learn_algo)\n",
    "\n",
    "#     precs = []\n",
    "#     recs = []\n",
    "#     n_pos = []\n",
    "#     for i in range(len(y_train)):\n",
    "#         model = LA.init_model(random_state=train_seed+i)\n",
    "#         model.fit(X_train[i],y_train[i])\n",
    "#         y_pred = model.predict(X_test[i])\n",
    "\n",
    "#         n_pos.append(np.sum(y_pred==1))\n",
    "#         precs.append(precision_score(y_test[i],y_pred))\n",
    "#         recs.append(recall_score(y_test[i],y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rng = default_rng(1)\n",
    "# n_obs = train_size + int(train_size * test_frac)\n",
    "# data_dict['train']['y'][0]\n",
    "# data_dict['train']['X']\n",
    "# bs_indices = rng.integers(0,high = n_obs,size = y.shape)\n",
    "\n",
    "# X = np.hstack([data_dict['train']['X'], data_dict['test']['X']])\n",
    "# y = np.hstack([data_dict['train']['y'], data_dict['test']['y']])\n",
    "\n",
    "# bs_indices\n",
    "\n",
    "# X_bs = np.zeros(X.shape)\n",
    "# y_bs = np.zeros(y.shape)\n",
    "\n",
    "# for i in range(len(y_bs)):\n",
    "#     X_bs[i] = X[i][bs_indices[i]]\n",
    "#     y_bs[i] = y[i][bs_indices[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs_indices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict['test']['X'][0].shape\n",
    "# np.vstack([data_dict['train']['X'][0],data_dict['test']['X'][0]]).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can use the Bayesian method by Caelen (2017); use a Multinomial likelihood for the confusion matrix and a Dirichlet prior on the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cms = Parallel(n_jobs = -1,verbose = 5)(delayed(f.holdout_CM)(model_name,train_size,test_frac,n_sets=n_sets,seed=seed_i) for seed_i in range(repeats))\n",
    "# cms = np.vstack(cms)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50e2ea73d98f6ee2d543936d9795e87546ceaa579e11054f6af1573a6cde0e6b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
