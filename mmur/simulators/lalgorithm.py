import warnings

import numpy as np
import xgboost as xgb
from joblib import Parallel,delayed
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.utils._testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.utils import check_random_state
from sklearn.utils import shuffle

from mmur.generators.blob_generator import BlobGenerator

class LAlgorithm():
    """
    Applies a learning algorithm to data generated by the generator
    """

    def __init__(
        self,
        model_name,
        data_generator, 
        random_state=123,
        penalty = 'none', 
        n_hidden_nodes = 100,
        nd_train = False,
        learning_rate = 0.005,
        n_estimators = 10,
        XGB_validation = True,
        val_frac = 0.2
        ):

        model_names = {'LR','NN','XGB'}
        if model_name not in model_names:
            raise NameError('Model name not one of ', model_names)

        self.model_name = model_name
        self.data_generator = data_generator
        self.rng = check_random_state(random_state)
        self.penalty = penalty
        self.n_hidden_nodes = n_hidden_nodes
        self.learning_rate = learning_rate
        self.nd_train = nd_train
        self.n_estimators = n_estimators
        self.validation = XGB_validation
        self.val_frac = val_frac

    def init_model(self,random_state=None):
        """Initialize the machine learning model. Optional seeding"""

        if random_state is not None:
            seeding = random_state
        else:
            seeding = self.rng

        if self.model_name == 'LR':
            self.model = LogisticRegression(penalty=self.penalty)
        if self.model_name == 'DT':
            self.model = DecisionTreeClassifier(random_state=seeding)
        if self.model_name == 'NN':
            self.model = MLPClassifier(hidden_layer_sizes=self.n_hidden_layers, learning_rate_init = self.learning_rate,
            random_state=seeding, shuffle = False)
        if self.model_name == 'XGB':
            self.model = xgb.XGBClassifier(random_state=seeding,n_estimators=self.n_estimators,eta=0.5)
        return self.model

    def unpack_data_dict(self,data_dict,stack = False):
        X_train = data_dict['train']['X']
        y_train = data_dict['train']['y']
        X_test = data_dict['test']['X']
        y_test = data_dict['test']['y']
        if stack:
            X = np.hstack([X_train, X_test])
            y = np.hstack([y_train, y_test])
            return X,y
        return X_train,X_test,y_train,y_test

    def predict_label(self,model,X_test,tau):
        probas = model.predict_proba(X_test)
        y_pred = (probas[:,1]>tau).astype(int)
        return y_pred

    def holdout_cm(self,model,data_dict,tau=0.5):
        X_train,X_test,y_train,y_test = self.unpack_data_dict(data_dict)

        if len(np.unique(y_train)) != self.data_generator.n_classes or len(np.unique(y_test)) != self.data_generator.n_classes:
            return None
            
        model = self.train_model(model,X_train,y_train)
        if model is None:
            return None

        y_pred = self.predict_label(model,X_test,tau)
        return confusion_matrix(y_test,y_pred,labels = [0,1]).tolist()

    def train_model(self,model,X_train,y_train):
        if self.nd_train:
            X_train, y_train = shuffle(X_train,y_train,random_state=self.rng)

        if self.model_name == 'NN':
            with warnings.catch_warnings():
                warnings.filterwarnings('error')
                try:
                    model.fit(X_train,y_train)
                except Warning:
                    print('Model did not converge')
                    return None

        if self.model_name == 'XGB' and self.validation:
            X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=int(len(y_train)*self.val_frac), random_state=self.rng)
            model.fit(X_train,y_train,eval_set = [(X_val,y_val)],early_stopping_rounds=5)

        else:
            model.fit(X_train,y_train)
        return model
        
    def pipeline(self,train_seed = None,random_state = None, tau=0.5):
        """Performs one run of the pipeline of the machine learning model, 
        optional seeding for each component"""
        if random_state is not None:
            self.data_generator.generator = check_random_state(random_state)
        #initialization
        model = self.init_model(train_seed)

        #data generation
        data_dict = self.data_generator.create_train_test()

        #model train and test
        cm = self.holdout_cm(model,data_dict,tau=tau)
        return cm

    def sim_true_cms(self,n_runs,n_jobs=15,train_seed=0):
        #parallel makes it that the generator is not updated --> need to have that within the function
        cms = Parallel(n_jobs=n_jobs,verbose=0)(delayed(self.pipeline)(random_state = i) for i in range(n_runs))

        #some runs have no occurrences of a certain class, delete those
        cms = list(filter(None, cms))

        self.true_cms = cms #save confusion matrices
        return cms

    #repeat nondeterministic training
    def repeat_nd_train(self,n_runs,data_seed = None,tau=0.5,parallel = False):
        """
        :n_runs: int, the number of train/test runs
        :data_seed: int, default = None, 
        If specified, a fixed dataset is guaranteed
        """

        if data_seed is not None:
            self.data_generator.generator = check_random_state(data_seed)
        data_dict = self.data_generator.create_train_test()

        if parallel:
            cms = Parallel(n_jobs=-1)(delayed(self.init_apply)(data_dict,tau,random_state=seed) for seed in range(n_runs)) 

        else:
            cms = []
            for _ in range(n_runs):
                model = self.init_model()
                cms.append(self.holdout_cm(model,data_dict,tau=tau))

        #remove failed runs
        cms = list(filter(None, cms))
        return cms

    def sim_prec_std(self,n_sets,n_runs,parallel = False):
        if parallel:
            stds = Parallel(n_jobs=-1)(delayed(self.calc_prec_std)(n_runs,data_seed=i) for i in range(n_sets))
        else:
            stds = []
            for _ in range(n_sets):
                stds.append(self.calc_prec_std(n_runs))
        return stds

    def sim_prec_iv(self,n_sets,n_runs,qrange=0.95,parallel = False,n_jobs=15):
        if parallel:
            iv_lens = Parallel(n_jobs=n_jobs)(delayed(self.calc_prec_iv)(n_runs,qrange=qrange,data_seed=i) for i in range(n_sets))
        else:
            iv_lens = []
            for _ in range(n_sets):
                iv_lens.append(self.calc_prec_iv(n_runs,qrange))
        return iv_lens

    def calc_prec_std(self,n_runs,data_seed = None):
        cms = self.repeat_nd_train(n_runs,data_seed=data_seed)
        precs = self.cms_to_precision(cms)
        return np.nanstd(precs)

    def calc_prec_iv(self,n_runs,qrange = 0.95,data_seed = None):
        cms = self.repeat_nd_train(n_runs,data_seed=data_seed)
        precs = self.cms_to_precision(cms)
        precs = precs[~np.isnan(precs)]
        q = (1-qrange)/2
        lower = np.quantile(precs,q)
        upper = np.quantile(precs,1-q)
        return upper-lower

    def init_apply(self,data_dict,tau,random_state=None):
        model = self.init_model(random_state=random_state)
        cm = self.holdout_cm(model,data_dict,tau)
        return cm

    def cms_to_precision(self,cms):
        cms_array = np.array(cms)
        TP = cms_array[:,1,1]
        FP = cms_array[:,0,1]
        return TP/(TP+FP)

    def cms_to_recall(self,cms):
        cms_array = np.array(cms)
        TP = cms_array[:,1,1]
        FN = cms_array[:,1,0]
        return TP/(TP+FN)

    

if __name__ == '__main__':
    import matplotlib.pyplot as plt
    data_generator = BlobGenerator(train_size=1000,test_size=200,weights = [0.8,0.2],random_imbalance=True)
    LA = LAlgorithm('XGB',data_generator,learning_rate=0.0001)
    

    LA.pipeline(0,0)
    # cms = []
    # for i in range(100):
    #     cms.append(LA.pipeline(train_seed=1,random_state=1))

    data_dict = data_generator.create_train_test()
    X_train,X_test,y_train,y_test = LA.unpack_data_dict(data_dict)

    nn_model = LA.init_model(1)

    print(LA.holdout_cm(nn_model,data_dict))
    # nn_model.fit(X_train,y_train)

    # plt.plot(nn_model.loss_curve_)
    # print(nn_model.learning_rate)
    # n_sets = 100
    # n_runs = 100
    # ivs= LA.sim_prec_iv(n_sets,n_runs,qrange=0.95,parallel = False,n_jobs=15)
    # print(ivs)