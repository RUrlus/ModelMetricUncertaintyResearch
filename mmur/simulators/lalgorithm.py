import warnings

import numpy as np
import xgboost as xgb
from joblib import Parallel, delayed
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.exceptions import ConvergenceWarning
from sklearn.utils import shuffle, check_random_state
from sklearn.utils._testing import ignore_warnings
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.model_selection import train_test_split

from mmur.generators.blob_generator import BlobGenerator


class LAlgorithm():
    """
    Applies a learning algorithm to data generated by the generator

    Parameters
    -------
    model_name : {'LR', 'NN', 'XGB'}
        The type of algorithm to be used. Logistic Regression (LR), Neural Network (NN) and XGBoost (XGB) 
        are currently supported. The first two use Sklearn's implementation, while the xgboost package is used for XGBoost.

    data_generator : obj
        The data generator or DGP used to sample data from. Currently only BlobGenerator is supported.

    nd_train : bool, default = False
        To allow for nondeterministic training. True implies that different training runs can produce different model fits.
        False implies that the same model fit is found for the same training set.

    random_state : int, RandomState instance or None, default = None
        Seed to allow for reproducible results.
    """

    def __init__(
        self,
        model_name,
        data_generator,
        nd_train=False,
        random_state=None,
        **kwargs
    ):

        model_names = {'LR', 'NN', 'XGB'}

        self.model_name = model_name
        self.data_generator = data_generator
        self.rng = check_random_state(random_state)
        self.nd_train = nd_train

        if self.model_name == 'LR':
            self.penalty = kwargs.get('penalty', 'none')
        elif self.model_name == 'NN':
            self.n_hidden_nodes = kwargs.get('n_hidden_nodes', 100)
            self.learning_rate = kwargs.get('learning_rate', 0.005)
        elif self.model_name == 'XGB':
            self.n_estimators = kwargs.get('n_estimators', 10)
            self.val_frac = kwargs.get('val_frac', None)
        else:
            raise NameError('Model name not one of ', model_names)

    def _init_model(self, random_state=None):
        """Initialize the machine learning model. Optional seeding"""

        if random_state is not None:
            seeding = random_state
        else:
            seeding = self.rng

        if self.model_name == 'LR':
            self.model = LogisticRegression(penalty=self.penalty)
        if self.model_name == 'DT':
            self.model = DecisionTreeClassifier(random_state=seeding)
        if self.model_name == 'NN':
            self.model = MLPClassifier(hidden_layer_sizes=self.n_hidden_nodes, learning_rate_init=self.learning_rate,
                                       random_state=seeding, shuffle=False)
        if self.model_name == 'XGB':
            self.model = xgb.XGBClassifier(
                random_state=seeding, n_estimators=self.n_estimators, eta=0.5)
        return self.model

    def _calc_prec_std(self, n_runs, data_seed=None):
        cms = self.repeat_nd_train(n_runs, data_seed=data_seed)
        precs = self.cms_to_precision(cms)
        return np.nanstd(precs)

    def _calc_prec_iv(self, n_runs, qrange=0.95, data_seed=None):
        cms = self.repeat_nd_train(n_runs, data_seed=data_seed)
        precs = self.cms_to_precision(cms)
        precs = precs[~np.isnan(precs)]
        q = (1-qrange)/2
        lower = np.quantile(precs, q)
        upper = np.quantile(precs, 1-q)
        return upper-lower

    def _init_apply(self, data_dict, tau, random_state=None):
        model = self._init_model(random_state=random_state)
        cm = self._holdout_cm(model, data_dict, tau)
        return cm

    def _cms_to_precision(self, cms):
        cms_array = np.array(cms)
        TP = cms_array[:, 1, 1]
        FP = cms_array[:, 0, 1]
        return TP/(TP+FP)

    def _cms_to_recall(self, cms):
        cms_array = np.array(cms)
        TP = cms_array[:, 1, 1]
        FN = cms_array[:, 1, 0]
        return TP/(TP+FN)

    def _unpack_data_dict(self, data_dict, stack=False):
        X_train = data_dict['train']['X']
        y_train = data_dict['train']['y']
        X_test = data_dict['test']['X']
        y_test = data_dict['test']['y']
        if stack:
            X = np.hstack([X_train, X_test])
            y = np.hstack([y_train, y_test])
            return X, y
        return X_train, X_test, y_train, y_test

    def _predict_label(self, model, X_test, tau):
        probas = model.predict_proba(X_test)
        y_pred = (probas[:, 1] > tau).astype(int)
        return y_pred

    def _holdout_cm(self, model, data_dict, tau=0.5):
        X_train, X_test, y_train, y_test = self._unpack_data_dict(data_dict)

        if len(np.unique(y_train)) != self.data_generator.n_classes or len(np.unique(y_test)) != self.data_generator.n_classes:
            return None

        model = self.train_model(model, X_train, y_train)
        if model is None:
            return None

        y_pred = self._predict_label(model, X_test, tau)
        return confusion_matrix(y_test, y_pred, labels=[0, 1]).tolist()

    def _train_model(self, model, X_train, y_train):
        if self.nd_train:
            X_train, y_train = shuffle(X_train, y_train, random_state=self.rng)

        if self.model_name == 'NN':
            with warnings.catch_warnings():
                warnings.filterwarnings('error')
                try:
                    model.fit(X_train, y_train)
                except Warning:
                    print('Model did not converge')
                    return None

        if self.model_name == 'XGB' and self.validation:
            X_train, X_val, y_train, y_val = train_test_split(
                X_train, y_train, test_size=int(len(y_train)*self.val_frac), random_state=self.rng)
            model.fit(X_train, y_train, eval_set=[
                      (X_val, y_val)], early_stopping_rounds=5)

        else:
            model.fit(X_train, y_train)
        return model

    def _pipeline(self, train_seed=None, random_state=None, tau=0.5):
        """Performs one run of the pipeline of the machine learning model, optional seeding for each component"""
        if random_state is not None:
            self.data_generator.generator = check_random_state(random_state)
        # initialization
        model = self._init_model(train_seed)

        # data generation
        data_dict = self.data_generator.create_train_test()

        # model train and test
        cm = self._holdout_cm(model, data_dict, tau=tau)
        return cm

    # TODO: add tau argument
    def sim_true_cms(self, n_runs, n_jobs=None, train_seed=0):
        """
        Simulates the "true" holdout set distribution of the confusion matrix.  
        A train and test set are generated, and the specified model is applied
        independently a specified number of times.


        Parameters
        ----------
        n_runs : int
            the number iterations, equal to the number of train/test samples to generate

        n_jobs : int, default = None
            number of CPU's used to perform computations in parallel (using joblib's 'Parallel' 
            and 'delayed' functions)

        Returns
        -------
        cms : List[List[int]]
            a list of all the confusion matrices
        """

        # parallel makes it that the generator is not updated --> need to have that within the function
        cms = Parallel(n_jobs=n_jobs, verbose=0)(
            delayed(self._pipeline)(random_state=i) for i in range(n_runs))

        # some runs have no occurrences of a certain class, delete those
        cms = list(filter(None, cms))

        self.true_cms = cms  # save confusion matrices
        return cms

    def repeat_nd_train(self, n_runs, data_seed=None, tau=0.5, n_jobs=None):
        """
        Repeats training and testing for the same data sample and split. Used to evaluate variations due to nondeterministic training

        Parameters
        -------
        n_runs : int, 
            the number of iterations of training and testing an algorithm on the same dataset and train/test split
        data_seed : int, default = None 
            if specified, a fixed dataset is guaranteed
        tau : float, default = 0.5
            the classification threshold, if the model output is higher than this value, the positive class (with label=1) is predicted, otherwise
            the negative class is predicted
        n_jobs : int, default = None 
            number of CPU's used to perform computations in parallel (using joblib's 'Parallel' 
            and 'delayed' functions)

        Returns
        -------
        cms : list of list of int
            the resulting confusion matrices corresponding to each run
        """

        if data_seed is not None:  # if data_seed is specified, set the seed of the data_generator instance
            self.data_generator.generator = check_random_state(data_seed)
        data_dict = self.data_generator.create_train_test()

        cms = Parallel(n_jobs=-1)(delayed(self._init_apply)(data_dict, tau, random_state=seed) for seed in range(n_runs))

        # remove failed runs
        cms = list(filter(None, cms))
        return cms

    def sim_prec_std(self, n_sets, n_runs, n_jobs = None):
        """
        Simulates the standard deviation of the precision due to nondeterministic training over random train/test splits

        Parameters
        -------
        n_sets : int
            number of independent train/test samples to test the algorithm on
        n_runs : int
            number of times repeat training and testing on a train/test sample to obtain a single estimate of the standard deviation
        n_jobs : int, default = None
            number of CPU's used to perform computations in parallel (using joblib's 'Parallel' 
            and 'delayed' functions)

        Returns
        -------
        stds : list of float
            the sample standard deviation for each train/test sample
        """
        stds = Parallel(n_jobs=-1)(delayed(self._calc_prec_std)(n_runs, data_seed=i) for i in range(n_sets))
        return stds

    def sim_prec_iv(self, n_sets, n_runs, qrange=0.95, n_jobs=None):
        """
        Simulates the standard deviation of the precision due to nondeterministic training over random train/test splits

        Parameters
        -------
        n_sets : int
            number of independent train/test samples to test the algorithm on
        n_runs : int
            number of times repeat training and testing on a train/test sample to obtain a single estimate of the standard deviation
        n_jobs : int, default = None
            number of CPU's used to perform computations in parallel (using joblib's 'Parallel' 
            and 'delayed' functions)

        Returns
        -------
        stds : list of float
            the sample standard deviation for each train/test sample
        """
        iv_lens = Parallel(n_jobs=n_jobs)(delayed(self._calc_prec_iv)(
            n_runs, qrange=qrange, data_seed=i) for i in range(n_sets))
        return iv_lens


if __name__ == '__main__':
    import matplotlib.pyplot as plt
    data_generator = BlobGenerator(train_size=1000, test_size=200, weights=[
                                   0.8, 0.2], random_imbalance=True)
    LA = LAlgorithm('XGB', data_generator, learning_rate=0.0001)

    LA._pipeline(0, 0)

    data_dict = data_generator.create_train_test()
    X_train, X_test, y_train, y_test = LA._unpack_data_dict(data_dict)

    nn_model = LA._init_model(1)

    print(LA._holdout_cm(nn_model, data_dict))