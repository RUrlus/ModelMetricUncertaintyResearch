from joblib import Parallel,delayed
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from logistic_generator import LogisticGenerator
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.utils import check_random_state
import numpy as np
from blob_generator import BlobGenerator


class LAlgorithm():
    """
    Applies a learning algorithm to data generated by the generator
    """

    def __init__(
        self,
        model_name,
        data_generator, 
        random_state=123,
        penalty = 'none', 
        n_hidden_layers = 5,
        fix_init = True
        ):

        model_names = {'LR','DT','NN','GB'}
        if model_name not in model_names:
            raise NameError('Model name not one of ', model_names)

        self.model_name = model_name
        self.data_generator = data_generator
        self.rng = check_random_state(random_state)
        self.penalty = penalty
        self.n_hidden_layers = n_hidden_layers
        self.fix_init = fix_init

    def init_model(self,random_state=0):
        """Initialize the machine learning model. Optional seeding"""

        if self.fix_init:
            seeding = random_state
        else:
            seeding = self.rng

        if self.model_name == 'LR':
            self.model = LogisticRegression(penalty=self.penalty)
        if self.model_name == 'DT':
            self.model = DecisionTreeClassifier(random_state=seeding)
        if self.model_name == 'NN':
            self.model = MLPClassifier(hidden_layer_sizes=self.n_hidden_layers, 
            random_state=seeding, shuffle = False)
        if self.model_name == 'GB':
            self.model = GradientBoostingClassifier(random_state=seeding)
        #random_state in the MLP fixes weight + bias initializations and batch sampling
        #might need to use Pytorch instead to allow more flexibility
        return self.model

    def unpack_data_dict(self,data_dict,stack = False):
        X_train = data_dict['train']['X']
        y_train = data_dict['train']['y']
        X_test = data_dict['test']['X']
        y_test = data_dict['test']['y']
        if stack:
            X = np.hstack([X_train, X_test])
            y = np.hstack([y_train, y_test])
            return X,y
        return X_train,X_test,y_train,y_test

    def predict_label(self,model,X_test,tau):
        probas = model.predict_proba(X_test)
        y_pred = (probas[:,1]>tau).astype(int)
        return y_pred

    def holdout_cm(self,model,data_dict,tau=0.5):
        X_train,X_test,y_train,y_test = self.unpack_data_dict(data_dict)
        # model = self.init_model(random_state=train_seed)
        model.fit(X_train,y_train)
        y_pred = self.predict_label(model,X_test,tau)
        return confusion_matrix(y_test,y_pred).tolist()

    def pipeline(self,train_seed = None,random_state = None, tau=0.5):
        """Performs one run of the pipeline of the machine learning model, 
        optional seeding for each component"""
        if random_state is not None:
            self.data_generator.generator = check_random_state(random_state)
        #initialization
        model = self.init_model(train_seed)

        #data generation
        data_dict = self.data_generator.create_train_test()
        # data_dict2 = self.data_generator.create_train_test()

        #model train and test
        cm = self.holdout_cm(model,data_dict,tau=tau)
        return cm

    def sim_true_cms(self,n_runs,n_jobs=-1,train_seed=0):
        #parallel makes it that the generator is not updated --> need to have that within the function
        cms = Parallel(n_jobs=n_jobs,verbose=1)(delayed(self.pipeline)(random_state = i) for i in range(n_runs))
        self.true_cms = cms #save confusion matrices
        return cms

    def cms_to_precision(self,cms):
        cms_array = np.array(cms)
        TP = cms_array[:,1,1]
        FP = cms_array[:,0,1]
        return TP/(TP+FP)

    def cms_to_recall(self,cms):
        cms_array = np.array(cms)
        TP = cms_array[:,1,1]
        FN = cms_array[:,1,0]
        return TP/(TP+FN)

if __name__ == '__main__':
    data_generator = BlobGenerator()
    LA = LAlgorithm('LR',data_generator)
    cms = LA.sim_true_cms(5)
    precs = LA.cms_to_precision(cms)
    # print(cm)